{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "CLINICAL_DATA = './IMPALA_Clinical_Data/Clean Data/IMPALA_Clinical_Data_202308211019_Raw.csv'\n",
    "CLINICAL_DTYPES = './IMPALA_Clinical_Data/IMPALAclinicalstudy_Dictionary_2023-09-25.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_clinical_df(path):\n",
    "    \"\"\" Load clinical data into a Pandas DataFrame. \"\"\"\n",
    "    return pd.read_csv(path, low_memory=False)\n",
    "\n",
    "\n",
    "def read_clinical_dtype_df(path):\n",
    "    \"\"\" Load clinical dict containing all variable names and they properties. \"\"\"\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "    df = df[df['Variable'].str.startswith(tuple(['recru', 'dly', 'record_id']))]\n",
    "\n",
    "    dtype_dict = defaultdict(list)\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        dtype_dict[row['Field Type']].append(row['Variable'])\n",
    "    \n",
    "    # {k: v.append('dly_time') for k, v in dtype_dict.items()}\n",
    "\n",
    "    return df, dtype_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clinical_df = read_clinical_df(CLINICAL_DATA)\n",
    "dtype_df, dtype_dict = read_clinical_dtype_df(CLINICAL_DTYPES)\n",
    "\n",
    "# Checkbox column names in dtype_df do not correspond 1-on-1 with clinical data\n",
    "checkbox_dict = {\n",
    "    'recru_hdu_admission_reason' : ['recru_resp', 'recru_circu', 'recru_neuro',\n",
    "                                    'recru_nurse', 'recru_unclear', 'recru_hdu_other'],\n",
    "    'recru_medication_specfy' : clinical_df.columns[clinical_df.columns.str.startswith('recru_medication_specfy___')].to_list(),\n",
    "    'dly_new_drug' : clinical_df.columns[clinical_df.columns.str.startswith('dly_new_drug___')].to_list(),\n",
    "}\n",
    "\n",
    "dtype_dict['checkbox'] = [l for s in checkbox_dict.values() for l in s]\n",
    "# dtype_dict['checkbox'].append('dly_time')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count CIE (CPR, PICU and death)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_cie_columns(df):\n",
    "    \"\"\"\n",
    "    Extract the colums that contain information about CIEs.\n",
    "    \"\"\"\n",
    "\n",
    "    cie_columns = {\n",
    "        'static'   : df.columns[df.columns.str.startswith(\n",
    "                        tuple(['recru_cie_other', 'recru_circulatory',\n",
    "                               'dis_outcome', 'dis_hospital_discharge',\n",
    "                               'dis_date_of_intervw',\n",
    "                               'recru_hospital_admission_date']))].to_list(),\n",
    "    \n",
    "        'temporal' : df.columns[df.columns.str.startswith(\n",
    "                        tuple(['dly_cie_other',\n",
    "                               'dly_circulatory']))].to_list()\n",
    "    }\n",
    "\n",
    "    cie_columns['temporal'].append('dly_time')\n",
    "\n",
    "    return cie_columns\n",
    "\n",
    "\n",
    "def split_per_patient(df, cie_columns):\n",
    "    \"\"\"\n",
    "    Extract the cie_columns from the original DataFrame and split it per patient.\n",
    "    \"\"\"\n",
    "\n",
    "    patient_dict = {}\n",
    "\n",
    "    for record_id, patient_df in df.groupby(['record_id'], observed=True):\n",
    "        patient_dict[record_id[0]] = {k : patient_df[v] for k, v in cie_columns.items()}\n",
    "    \n",
    "    return patient_dict\n",
    "\n",
    "\n",
    "def clean_cie_dataframes(patient_dict, freq='T'):\n",
    "    \"\"\"\n",
    "    Clean the CIE information captured in the patient_dict. Since some variables\n",
    "    are temporal and others static the need to be processed differently.\n",
    "\n",
    "    :param patient_dict: a dictionary containing DataFrames with static\n",
    "                         and temporal information regarding CIEs, per patient.\n",
    "    :param freq: time unit to which dtes are rounded (T = minutes, H = hours).\n",
    "    :return patient_dict: the cleaned input dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    for record_id, patient_df in patient_dict.items():\n",
    "        static = patient_df['static']\n",
    "        temporal = patient_df['temporal']\n",
    "        \n",
    "        ### Clean static DataFrame ###\n",
    "        static = static.iloc[0, :].to_dict()\n",
    "\n",
    "        # If no exact discharge time is given, use discharge date\n",
    "        if pd.isna(static['dis_hospital_discharge']):\n",
    "            static['dis_hospital_discharge'] = static['dis_date_of_intervw']\n",
    "            del static['dis_date_of_intervw']\n",
    "        \n",
    "        # Turn string date into Pandas TimeStamp\n",
    "        static['dis_hospital_discharge'] = pd.to_datetime(\n",
    "            static['dis_hospital_discharge'],\n",
    "            format='mixed').round(freq=freq)\n",
    "        \n",
    "        static['recru_hospital_admission_date'] = pd.to_datetime(\n",
    "            static['recru_hospital_admission_date'],\n",
    "            format='mixed').round(freq=freq)\n",
    "        \n",
    "        # Replace NaN values with 0\n",
    "        static = {k : (0 if pd.isna(v) else v) for k, v in static.items()}\n",
    "\n",
    "        patient_dict[record_id]['static'] = static\n",
    "\n",
    "        ### Clean temporal DataFrame ###\n",
    "        # Set daily time as index \n",
    "        temporal.set_index('dly_time', inplace=True)\n",
    "        temporal = temporal[temporal.index.notnull()]\n",
    "        temporal.index = pd.to_datetime(temporal.index, format='mixed').round(freq=freq)\n",
    "\n",
    "        # Replace NaN values with 0\n",
    "        temporal = temporal.fillna(value=0)\n",
    "\n",
    "        patient_dict[record_id]['temporal'] = temporal\n",
    "\n",
    "\n",
    "    return patient_dict\n",
    "\n",
    "\n",
    "def process_cpr(dfs):\n",
    "    \"\"\"\n",
    "    Count CPR occurences of a patient.\n",
    "    \"\"\"\n",
    "\n",
    "    # Recruitment\n",
    "    time = dfs['static']['recru_hospital_admission_date']\n",
    "    n_recru_cpr = sum([v for k, v in dfs['static'].items() if 'circulatory' in k and v == 4])\n",
    "    recru_cpr = [time] if n_recru_cpr > 0 else []\n",
    "    \n",
    "    # Daily\n",
    "    dly_cpr_df = dfs['temporal'][dfs['temporal'].columns[dfs['temporal'].columns.str.startswith('dly_circulatory')]]\n",
    "    dly_cpr = [time for time, row in dly_cpr_df.iterrows() if (row == 4).any()]\n",
    "\n",
    "\n",
    "    dly_cpr += recru_cpr\n",
    "\n",
    "    return dly_cpr\n",
    "\n",
    "\n",
    "def process_picu(dfs):\n",
    "    \"\"\"\n",
    "    Count PICU occurences of a patient.\n",
    "    \"\"\"\n",
    "\n",
    "    # Recruitment\n",
    "    time = dfs['static']['recru_hospital_admission_date']\n",
    "    n_recru_picu = sum([v for k, v in dfs['static'].items() if 'cie_other' in k and v == 2])\n",
    "    recru_picu = [time] if n_recru_picu > 0 else []\n",
    "    \n",
    "    # Daily\n",
    "    dly_picu_df = dfs['temporal'][dfs['temporal'].columns[dfs['temporal'].columns.str.startswith('dly_cie_other')]]\n",
    "    dly_picu = [time for time, row in dly_picu_df.iterrows() if (row == 2).any()]\n",
    "\n",
    "    dly_picu += recru_picu\n",
    "\n",
    "    return dly_picu\n",
    "\n",
    "\n",
    "def process_death(dfs):\n",
    "    \"\"\"\n",
    "    Count death occurences of a patient.\n",
    "    \"\"\"\n",
    "\n",
    "    # Recruitment\n",
    "    time = dfs['static']['recru_hospital_admission_date']\n",
    "    n_recru_death = sum([v for k, v in dfs['static'].items() if 'cie_other' in k and v == 4])\n",
    "    recru_death = [time] if n_recru_death > 0 else []\n",
    "    \n",
    "    # Discharge\n",
    "    time = dfs['static']['dis_hospital_discharge']\n",
    "    dis_death = [time] if dfs['static']['dis_outcome'] == 2 else []\n",
    "\n",
    "    # Daily\n",
    "    dly_death_df = dfs['temporal'][dfs['temporal'].columns[dfs['temporal'].columns.str.startswith('dly_cie_other')]]\n",
    "    dly_death = [time for time, row in dly_death_df.iterrows() if (row == 4).any()]\n",
    "\n",
    "    dly_death += recru_death\n",
    "    dly_death += dis_death\n",
    "\n",
    "    # Since someone can only die once, so choose the oldest date\n",
    "    if dly_death != []:\n",
    "        return [min(dly_death)]\n",
    "    else:\n",
    "        return dly_death\n",
    "\n",
    "\n",
    "def get_cie_timepoints(patient_dict, save_file=False):\n",
    "    \"\"\"\n",
    "    For each patient, create a dictionary that contains timepoints of the CIEs\n",
    "    that (possibly) occured.\n",
    "    :param patient_dict: a dictionary containing DataFrames with static and\n",
    "                         temporal information regarding CIEs, per patient.\n",
    "    :param save_file: flag whether the resulting CIE dictionary should be saved\n",
    "                      to a file.csv\n",
    "    :return cie_dict: a dictionary containing timepoints of CIEs per patient.\n",
    "    \"\"\"\n",
    "\n",
    "    cie_dict = defaultdict(dict)\n",
    "\n",
    "    for record_id, patient_df in patient_dict.items():\n",
    "\n",
    "        cie_dict[record_id]['cpr'] = process_cpr(patient_df)\n",
    "\n",
    "        cie_dict[record_id]['picu'] = process_picu(patient_df)\n",
    "\n",
    "        cie_dict[record_id]['death'] = process_death(patient_df)\n",
    "    \n",
    "    if save_file:\n",
    "        with open('CIE_dict', 'wb') as file:\n",
    "            pickle.dump(cie_dict, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return cie_dict\n",
    "\n",
    "\n",
    "def get_all_cie(df, freq='T', save_file=False, show_count=True):\n",
    "    \"\"\"\n",
    "    Create a dictionary containing the timepoints all CIEs per patient\n",
    "    \"\"\"\n",
    "\n",
    "    cie_columns = get_cie_columns(df)\n",
    "\n",
    "    patient_dict = split_per_patient(df, cie_columns)\n",
    "\n",
    "    patient_dict = clean_cie_dataframes(patient_dict, freq=freq)\n",
    "\n",
    "    cie_dict = get_cie_timepoints(patient_dict, save_file=save_file)\n",
    "\n",
    "    if show_count:\n",
    "        \n",
    "        cpr, picu, death = 0, 0, 0\n",
    "        for d in cie_dict.values():\n",
    "            cpr += len(d['cpr'])\n",
    "            picu += len(d['picu'])\n",
    "            death += len(d['death'])\n",
    "\n",
    "        print(\"=== Critical Illness Events ===\")\n",
    "        print(f\"- CPR  : {cpr}\")\n",
    "        print(f\"- PICU : {picu}\")\n",
    "        print(f\"- Death: {death}\")\n",
    "        print(f\"- Total: {cpr + picu + death}\")\n",
    "\n",
    "    del patient_dict\n",
    "\n",
    "    return cie_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cie_dict = get_all_cie(clinical_df, freq='T', save_file=False, show_count=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare DataType data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def intersection(l1, l2):\n",
    "    \"\"\" Create intersection of two lists. \"\"\"\n",
    "    return [v1 for v1 in l1 if v1 in l2]\n",
    "\n",
    "\n",
    "def replace_missing_values(df, threshold=None, freq='T'):\n",
    "    \"\"\"\n",
    "    Clean the missing/NaN values by replacing them with -1. If a threshold is\n",
    "    set, remove columns where missing/NaN value ratio exceeds this threshold.\n",
    "    Also set the daily time as the index for each row.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Remove rows that have no daily time (as those are duplicates of the row above)\n",
    "    # df = df[-pd.isna(df['dly_time'])]\n",
    "\n",
    "    # Set daily time as DataFrame index\n",
    "    # df.set_index('dly_time', inplace=True)\n",
    "    # df.index = pd.to_datetime(df.index, format='mixed').round(freq=freq)\n",
    "\n",
    "    # Convert numbers to float\n",
    "    df = df.astype(float)\n",
    "\n",
    "    # Replace missing/NaN entries\n",
    "    df.replace(to_replace=99, value=-1, inplace=True)\n",
    "    df.fillna(-1, inplace=True)\n",
    "\n",
    "    # Remove columns where missing/NaN values exceed threshold\n",
    "    if threshold:\n",
    "        for col in df:\n",
    "            if -1 in df[col].value_counts(normalize=True).to_dict().keys() and \\\n",
    "                    df[col].value_counts(normalize=True).to_dict()[-1] > threshold:\n",
    "\n",
    "                df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def one_hot_encoding(df):\n",
    "    \"\"\"\n",
    "    Convert categorical columns to multiple binary columns and add them to the\n",
    "    end of the DataFrame. Remove binary columns that are created for\n",
    "    missing/NaN values.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    categorical_columns = df.columns.to_list()\n",
    "\n",
    "    for col in tqdm(categorical_columns):\n",
    "\n",
    "        if -1 in df[col].value_counts().to_dict().keys():\n",
    "            DROP_FIRST = True\n",
    "\n",
    "        else:\n",
    "            DROP_FIRST = False\n",
    "\n",
    "        df = df.join(pd.get_dummies(df[col],\n",
    "                                    dummy_na=False, \n",
    "                                    prefix=col,\n",
    "                                    sparse=True,\n",
    "                                    drop_first=DROP_FIRST,\n",
    "                                    dtype=float))\n",
    "\n",
    "        # Drop original categorical column\n",
    "        df = df.drop(col, axis=1)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clinical_df = read_clinical_df(CLINICAL_DATA)\n",
    "clinical_df.fillna(-1, inplace=True)\n",
    "\n",
    "valid_radio_columns = intersection(dtype_dict['radio'], clinical_df.columns.to_list())\n",
    "valid_yesno_columns = intersection(dtype_dict['yesno'], clinical_df.columns.to_list())\n",
    "valid_checkbox_columns = intersection(dtype_dict['checkbox'], clinical_df.columns.to_list())\n",
    "valid_text_columns = intersection(dtype_dict['text'], clinical_df.columns.to_list())\n",
    "\n",
    "radio_df = clinical_df[valid_radio_columns]\n",
    "yesno_df = clinical_df[valid_yesno_columns]\n",
    "checkbox_df = clinical_df[valid_checkbox_columns]\n",
    "\n",
    "text_df = clinical_df[valid_text_columns]\n",
    "num_df = text_df.select_dtypes(include=['float64', 'int64'])\n",
    "valid_num_columns = num_df.columns.to_list()\n",
    "text_df = text_df.drop(valid_num_columns, axis=1)\n",
    "valid_text_columns = [col for col in valid_text_columns if col not in valid_num_columns]\n",
    "\n",
    "# print(radio_df.memory_usage().sum())\n",
    "\n",
    "radio_df = radio_df.astype(np.int16)\n",
    "yesno_df = yesno_df.astype(np.int16)\n",
    "checkbox_df = checkbox_df.astype(np.int16)\n",
    "\n",
    "radio_df = replace_missing_values(radio_df)\n",
    "radio_df = one_hot_encoding(radio_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Prinicipal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def perform_PCA(df, visualize=True):\n",
    "    \"\"\"\n",
    "    Standardize the data and perform PCA to reduce dimensionality.\n",
    "    If visualize is True, plot the explained variance per PC.\n",
    "    \"\"\"\n",
    "\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df.values)\n",
    "    data = scaler.transform(df.values)\n",
    "\n",
    "    # Perform Principal Component Analysis\n",
    "    pca = PCA(n_components='mle')\n",
    "    pca.fit(data)\n",
    "    new_data = pca.transform(data)\n",
    "\n",
    "    if visualize:\n",
    "        plt.plot(range(pca.n_components_), pca.explained_variance_ratio_)\n",
    "        plt.title('Explained variance ratio per principal component')\n",
    "        plt.xlabel('Number of components')\n",
    "        plt.ylabel('Ratio of explained variance')\n",
    "        plt.legend(['Explained Variance', \"Number of PCs found using Minka's MLE\"])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return new_data, pca\n",
    "\n",
    "\n",
    "# PCA visualization\n",
    "def plot_contribution(components, idx, columns, top_n=50):\n",
    "    \n",
    "    sorted_idx = np.argsort(-components[idx, :])\n",
    "\n",
    "    sorted_components = components[idx, :][sorted_idx]\n",
    "    sorted_columns = columns[sorted_idx]\n",
    "    \n",
    "    if top_n:\n",
    "        plt.bar(sorted_columns[:top_n], sorted_components[:top_n])\n",
    "    else:\n",
    "        plt.bar(sorted_columns[:top_n], sorted_components[:top_n])\n",
    "\n",
    "    plt.xticks(rotation=-90, ha='center')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Contribution in component')\n",
    "    plt.title(f'Contributions for principal component {idx+1}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return sorted_components, sorted_columns\n",
    "\n",
    "\n",
    "def calculate_most_contribution_component(components, columns, top_n=10, top_m=10):\n",
    "    \"\"\"\n",
    "    Count the top 10 features for each component and rank the top 10.\n",
    "    \"\"\"\n",
    "\n",
    "    counter = defaultdict(int)\n",
    "\n",
    "    for component in components:\n",
    "\n",
    "        sorted_idx = np.argsort(-component)\n",
    "        sorted_component = component[sorted_idx]\n",
    "        sorted_columns = columns[sorted_idx]\n",
    "\n",
    "        for i in range(top_n):\n",
    "            counter[sorted_columns[i]] += sorted_component[i]\n",
    "\n",
    "\n",
    "    counter = dict(Counter(counter).most_common(top_m))\n",
    "\n",
    "    plt.bar(counter.keys(), counter.values())\n",
    "\n",
    "    plt.xticks(rotation=-90, ha='center')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Contribution in component')\n",
    "    plt.title('Most contributing features over all components')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_data, pca = perform_PCA(radio_df)\n",
    "\n",
    "calculate_most_contribution_component(pca.components_, radio_df.columns.to_numpy(), top_n=100, top_m=25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sliding_window_old(patient_dict, cie_dict, sample_window_hours, predictive_window_hours):\n",
    "    \"\"\"\n",
    "    Apply a sliding window to the patient's data. Iterate over the timepoints:\n",
    "    apply a sliding window (sample window) starting with the current timepoint,\n",
    "    see what other timepoints fit into that window and finally append another\n",
    "    window (predictive window) after the sample window to see if a CIE occurs\n",
    "    in the future.\n",
    "    :param patient_dict: a dictionary containing DataFrames with static and\n",
    "                         temporal information regarding CIEs, per patient.\n",
    "    :param cie_dict: a dictionary containing timepoints of CIEs per patient.\n",
    "    :param sample_window_hours: an integer indicating the size/hours of the\n",
    "                                sliding window.\n",
    "    :param predictive_window_hours: an integer indicating the size/hours of the\n",
    "                                predictive window.\n",
    "    \"\"\"\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for i, (record_id, patient_df) in enumerate(patient_dict.items()):\n",
    "\n",
    "        if i > 10:\n",
    "            break\n",
    "\n",
    "        # Static data\n",
    "\n",
    "        # Temporal data\n",
    "        for start in patient_df['temporal'].index:\n",
    "\n",
    "            # Create sample window (data sample)\n",
    "            sample_end = start + pd.to_timedelta(sample_window_hours, unit='h')\n",
    "            s_timepoints = patient_df['temporal'].index.to_series().between(start, sample_end)\n",
    "            sample_window = patient_df['temporal'][s_timepoints]\n",
    "\n",
    "            # Check if CIE occurs during predictive window (output)\n",
    "            predictive_end = sample_end + pd.to_timedelta(predictive_window_hours, unit='h')\n",
    "            all_cie = [sum([1 for t in d if t >= sample_end and t <= predictive_end]) \\\n",
    "                       for d in cie_dict[record_id].values()]\n",
    "\n",
    "            \n",
    "            X.append(sample_window.values)\n",
    "            y.append(all_cie)\n",
    "\n",
    "    # X = np.array(X) # shape: data samples, timepoints, dimensions\n",
    "    # y = np.array(y) # shape: data samples, number of CIE (outputs)\n",
    "\n",
    "\n",
    "    return X, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sliding_window(df, cie_dict, sample_window_hours, predictive_window_hours):\n",
    "    pass\n",
    "\n",
    "\n",
    "print(dtype_dict['checkbox'])\n",
    "\n",
    "# display(radio_df['dly_time'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
