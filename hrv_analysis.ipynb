{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import seaborn as sns\n",
    "from dateutil.rrule import rrule, SECONDLY, MINUTELY, HOURLY\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from preprocessing.load_raw_vital_signs import *\n",
    "from preprocessing.heart_rate_variability import calculate_hrv\n",
    "\n",
    "RAW_VITAL_DATA_PATH = \"./DATA/Raw Data/filtered_df_removed_nan_files.parquet\"\n",
    "CLINICAL_DATA = './DATA/Clean Data/IMPALA_Clinical_Data_202308211019_Raw.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select patients based on criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_clinical_df(path):\n",
    "    \"\"\" Load clinical data into a Pandas DataFrame. \"\"\"\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "    return df[['record_id', 'dis_outcome']]\n",
    "\n",
    "def select_patients_by_los(df, lower_bound_hours, upper_bound_hours, n_selected_patients=None):\n",
    "    \"\"\"\n",
    "    Select a number of patients based on their length of stay.\n",
    "    \"\"\"\n",
    "\n",
    "    seed = 42\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    alive_ids, died_ids = [], []\n",
    "\n",
    "    for patient_id, series in df.groupby('record_id'):\n",
    "\n",
    "        if series.shape[0] >= np.ceil(lower_bound_hours/4) and \\\n",
    "                series.shape[0] <= np.ceil(upper_bound_hours/4):\n",
    "            \n",
    "            if series['dis_outcome'].iloc[0] == 1:\n",
    "                alive_ids.append(patient_id)\n",
    "            elif series['dis_outcome'].iloc[0] == 2:\n",
    "                died_ids.append(patient_id)\n",
    "\n",
    "    print(f'====== Select patients that fit criteria (min {lower_bound_hours}h, max {upper_bound_hours}h) ========')\n",
    "    print(f'- Number of alive patients: {len(alive_ids)}')\n",
    "    print(f'- Number of deceased patients: {len(died_ids)}\\n')\n",
    "\n",
    "    max_n = min(len(alive_ids), len(died_ids))\n",
    "    if not n_selected_patients or n_selected_patients > max_n:\n",
    "        n_selected_patients = max_n\n",
    "\n",
    "    alive_ids = np.random.choice(alive_ids, size=n_selected_patients, replace=False)\n",
    "    died_ids = np.random.choice(died_ids, size=n_selected_patients, replace=False)\n",
    "\n",
    "    print(f'====== Randomly select {n_selected_patients} patients from both selected groups =======')\n",
    "    print(f'Alive patients\\n{alive_ids}\\n')\n",
    "    print(f'Deceased patients\\n{died_ids}')\n",
    "\n",
    "    return alive_ids, died_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = read_clinical_df(CLINICAL_DATA)\n",
    "\n",
    "alive_ids_short, died_ids_short = select_patients_by_los(df, 1, 13)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Read data\n",
    "# data_short = read_raw_vital_signs(RAW_VITAL_DATA_PATH, batch_size=10000, patient_id=list(np.concatenate((alive_ids_short, died_ids_short))))\n",
    "\n",
    "# ### Save data\n",
    "# save_patient_dict(data_short, './DATA/Raw Data/raw_patient_dict_short')\n",
    "\n",
    "# ### Load data\n",
    "# data_long = load_patient_dict('./DATA/Raw Data/raw_patient_dict_p30')\n",
    "\n",
    "print(data_short.keys())\n",
    "print(data_long.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heart rate variability\n",
    "\n",
    "| Frequency-domain | Time-domain | Non linear-domain |\n",
    "|:---:|:---:|:---:|\n",
    "| lf, hf, lf_hf_ratio, lfnu, hfnu, total_power, vlf | mean_nni, sdnn, sdsd, nni_50, pnni_50, nni_20, pnni_20, rmssd, median_nni, range_nni, cvsd, cvnni, mean_hr, max_hr, min_hr, std_hr | csi, cvi, Modified_csi |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_data_into_windows(df, time_unit='m', time_freq=15):\n",
    "    \"\"\"\n",
    "    Clean and split the data into windows.\n",
    "    :param df: Pandas DataFrame containing the data indexed on timestamps.\n",
    "    :param time_unit: time unit of the data window, e.g. s (seconds), m (minutes).\n",
    "    :param time_freq: number of time units in the data window.\n",
    "    \"\"\"\n",
    "\n",
    "    # Clean data\n",
    "    df = df[['ECGHR', 'datetime']]\n",
    "    df = df.set_index('datetime')\n",
    "    df = df.sort_index()\n",
    "\n",
    "    # Split data\n",
    "    rrule_time = {'h' : HOURLY, 'm' : MINUTELY, 's' : SECONDLY}\n",
    "    windows = []\n",
    "    timestamps = []\n",
    "    \n",
    "    for start in rrule(freq=rrule_time[time_unit], interval=time_freq,\n",
    "                       dtstart=df.index[0], until=df.index[-1]):\n",
    "        \n",
    "        end = start + pd.Timedelta(time_freq, unit=time_unit)\n",
    "        idx = df.index.to_series().between(start, end)\n",
    "        if len(idx) > 0:\n",
    "            window = df[idx]\n",
    "\n",
    "            windows.append(window.values.squeeze())\n",
    "            timestamps.append(end)\n",
    "\n",
    "    return windows, timestamps\n",
    "\n",
    "\n",
    "def get_hrv_scores(data, normalize_data=False):\n",
    "    \"\"\"\n",
    "    Calculate the heart rate variability of all patients.\n",
    "    \"\"\"\n",
    "\n",
    "    hrv_data = defaultdict(list)\n",
    "    time_data = defaultdict(list)\n",
    "    threshold = 280 # 5 minutes of data with a error margin of 20 seconds\n",
    "    error = 0\n",
    "    total = 0\n",
    "\n",
    "    for patient_id, df in data.items():\n",
    "\n",
    "        if df.shape[0] == 0:\n",
    "            print(f'Patient {patient_id} has not data entries')\n",
    "            continue\n",
    "\n",
    "        windows, timestamps = split_data_into_windows(df)\n",
    "\n",
    "        for window, time in zip(windows, timestamps):\n",
    "\n",
    "            total += 1\n",
    "\n",
    "            if np.where(np.isnan(window), 0, 1).sum() >= threshold:\n",
    "\n",
    "                hrv_features = calculate_hrv(window)\n",
    "\n",
    "                # Check if hrv results only contain valid scores\n",
    "                if hrv_features and ~np.isnan(np.array(list(hrv_features.values()))).any():\n",
    "                    hrv_data[patient_id].append(list(hrv_features.values()))\n",
    "                    time_data[patient_id].append(time)\n",
    "                    continue\n",
    "\n",
    "            error += 1\n",
    "\n",
    "    # Normalize each feature to be between 0 and 1\n",
    "    if normalize_data:\n",
    "        hrv_data = {k : normalize(np.array(v), axis=0, norm='max') for k, v in hrv_data.items()}\n",
    "    else:\n",
    "        hrv_data = {k : np.array(v) for k, v in hrv_data.items()}\n",
    "\n",
    "    time_data = {k : np.array(v) for k, v in time_data.items()}\n",
    "\n",
    "    print(f'{round(error / total * 100, 2)}% of windows were invalid')\n",
    "\n",
    "    return hrv_data, time_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NAMES = np.array(['lf', 'hf', 'lf_hf_ratio', 'lfnu', 'hfnu', 'total_power', 'vlf',\n",
    "                  'mean_nni', 'sdnn', 'sdsd', 'nni_50', 'pnni_50', 'nni_20', 'pnni_20',\n",
    "                  'rmssd', 'median_nni', 'range_nni', 'cvsd', 'cvnni', 'mean_hr',\n",
    "                  'max_hr', 'min_hr', 'std_hr', 'csi', 'cvi', 'Modified_csi'])\n",
    "\n",
    "hrv_data_long, time_data_long = get_hrv_scores(data_long, normalize_data=True)\n",
    "hrv_data_short, time_data_short = get_hrv_scores(data_short, normalize_data=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_data_per_category(hrv_data, alive_ids, died_ids):\n",
    "    \"\"\"\n",
    "    Split the hrv data between alive and died patients.\n",
    "    \"\"\"\n",
    "\n",
    "    data_alive = np.concatenate([v for k, v in hrv_data.items() if k in alive_ids])\n",
    "    data_died = np.concatenate([v for k, v in hrv_data.items() if k in died_ids])\n",
    "    data_all = np.concatenate((data_alive, data_died))\n",
    "\n",
    "    return data_alive, data_died, data_all\n",
    "\n",
    "\n",
    "def choose_last_hours(data, time_data, n_hours=12):\n",
    "    \"\"\"\n",
    "    Choose the final N hours of the data.\n",
    "    \"\"\"\n",
    "\n",
    "    new_data = dict()\n",
    "    new_time = dict()\n",
    "\n",
    "    for (patient_id, scores), time in zip(data.items(), time_data.values()):\n",
    "\n",
    "        end = time[-1]\n",
    "        start = end - pd.Timedelta(n_hours, unit='h')\n",
    "        idx = [True if d >= start and d <= end else False for d in time]\n",
    "\n",
    "        new_data[patient_id] = scores[idx]\n",
    "        new_time[patient_id] = time[idx]\n",
    "    \n",
    "    return new_data, new_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alive_ids_long = ['Z-H-0182', 'Z-H-0155', 'Z-H-0336', 'Z-H-0290', 'B-S-0007', 'Z-H-0373',\n",
    "             'B-N-0063', 'B-S-0159', 'Z-H-0376', 'Z-H-0130', 'Z-H-0144', 'B-S-0166',\n",
    "             'Z-H-0044', 'Z-H-0173', 'B-S-0151']\n",
    "\n",
    "died_ids_long = ['Z-H-0114', 'B-N-0080', 'B-N-0084', 'Z-H-0348', 'Z-H-0198', 'Z-H-0032',\n",
    "            'Z-H-0350', 'Z-H-0185', 'Z-H-0054', 'B-N-0058', 'B-S-0242', 'Z-H-0120',\n",
    "            'Z-H-0308', 'B-S-0292', 'Z-H-0116']\n",
    "\n",
    "# All data\n",
    "# data_alive, data_died, data_all = split_data_per_category(hrv_data, alive_ids, died_ids)\n",
    "\n",
    "# Selected data\n",
    "hrv_data_12h_long, time_data_12h_long = choose_last_hours(hrv_data_long, time_data_long, n_hours=12)\n",
    "data_alive_12h_long, data_died_12h_long, data_all_12h_long = split_data_per_category(hrv_data_12h_long, alive_ids_long, died_ids_long)\n",
    "\n",
    "hrv_data_12h_short, time_data_12h_short = choose_last_hours(hrv_data_short, time_data_short, n_hours=12)\n",
    "data_alive_12h_short, data_died_12h_short, data_all_12h_short = split_data_per_category(hrv_data_12h_short, alive_ids_short, died_ids_short)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sklearn methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def feature_selection(data_alive, data_died, n_seeds=10, visualize=True):\n",
    "    \"\"\"\n",
    "    Perform feature selection based on 3 classifiers and 2 methods from Sklearn.\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare data\n",
    "    y_alive = np.ones(data_alive.shape[0])\n",
    "    y_died = np.zeros(data_died.shape[0])\n",
    "    X = np.concatenate((data_alive, data_died))\n",
    "    y = np.concatenate((y_alive, y_died))\n",
    "\n",
    "    feature_counter = np.zeros(X.shape[1])\n",
    "\n",
    "    for seed in range(n_seeds):\n",
    "\n",
    "        classifiers = [LogisticRegression(max_iter=10000, random_state=seed),\n",
    "                       LinearSVC(dual='auto',max_iter=10000, random_state=seed),\n",
    "                       ExtraTreesClassifier(random_state=seed)]\n",
    "        \n",
    "        for clf in classifiers:\n",
    "            # Select using feature importance\n",
    "            clf_fit = clf.fit(X, y)\n",
    "            clf_model = SelectFromModel(clf_fit, prefit=True, max_features=10)\n",
    "            feature_counter += clf_model.get_support().astype(int)\n",
    "\n",
    "            # Recursive Feature Elimination\n",
    "            clf_rfe = RFE(estimator=clf, n_features_to_select=10).fit(X, y)\n",
    "            feature_counter += clf_rfe.get_support().astype(int)\n",
    "    \n",
    "    \n",
    "    if visualize:\n",
    "        plt.bar(range(7), feature_counter[:7])\n",
    "        plt.bar(range(7, 23), feature_counter[7:23])\n",
    "        plt.bar(range(23, 26), feature_counter[23:26])\n",
    "        plt.xticks(range(len(NAMES)), NAMES, rotation=-90)\n",
    "        plt.ylabel('N times in the 10')\n",
    "        plt.title(f'Features selection (3 classifiers, 2 selectors, {n_seeds} seeds)')\n",
    "        plt.legend(['Frequency-domain', 'Time-domain', 'Non Linear-domain'])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return np.argsort(-feature_counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sorted_features = feature_selection(data_alive_12h_long, data_died_12h_long)\n",
    "print(sorted_features)\n",
    "\n",
    "sorted_features = feature_selection(data_alive_12h_short, data_died_12h_short)\n",
    "print(sorted_features)\n",
    "\n",
    "sorted_features = feature_selection(data_died_12h_long, data_died_12h_short)\n",
    "print(sorted_features)\n",
    "\n",
    "sorted_features = feature_selection(data_alive_12h_long, data_alive_12h_short)\n",
    "print(sorted_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_histograms(alive, died):\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    j = 0\n",
    "\n",
    "    for i, feature in enumerate(NAMES):\n",
    "\n",
    "        if i in [2, 3, 7, 9, 14, 15, 20, 24]:\n",
    "\n",
    "            plt.subplot(3, 3, j+1)\n",
    "            j += 1\n",
    "\n",
    "            sns.histplot(alive[:, i],\n",
    "                        kde=False, bins=50, color='tab:blue')\n",
    "            sns.histplot(died[:, i],\n",
    "                        kde=False, bins=50, color='tab:orange')\n",
    "            \n",
    "            # sns.histplot(alive[:, i][alive[:, i] <= 0.2],\n",
    "            #              kde=True, bins=50, color='tab:blue')\n",
    "            # sns.histplot(died[:, i][died[:, i] <= 0.2],\n",
    "            #              kde=True, bins=50, color='tab:orange')\n",
    "            # plt.xlim(-0.01, 0.21)\n",
    "\n",
    "            plt.title(feature)\n",
    "            plt.legend(['Alive', 'Deceased'])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_histograms(data_alive_12h, data_died_12h)\n",
    "# plot_histograms(data_alive, data_died)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scores over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "time_data_1 = time_data_12h_long['Z-H-0114']\n",
    "time_data_2 = time_data_12h_long['Z-H-0182']\n",
    "hrv_data_1 = hrv_data_12h_long['Z-H-0114']\n",
    "hrv_data_2 = hrv_data_12h_long['Z-H-0182']\n",
    "\n",
    "feature_idx = range(5) #[2, 3, 7, 9, 14, 15, 20, 24]\n",
    "\n",
    "# Make times equal\n",
    "new_time_died = [(datetime.datetime.now() + (t - time_data_1[0])) for t in time_data_1]\n",
    "new_time_alive = [(datetime.datetime.now() + (t - time_data_2[0])) for t in time_data_2]\n",
    "\n",
    "# Plot graphs\n",
    "fig, ax = plt.subplots(len(feature_idx), 1, figsize=(15, len(feature_idx)*3), sharex=True)\n",
    "ticks = [0, int(len(new_time_died)/4), int(len(new_time_died)/2), int(3*(len(new_time_died)/4)), -1]\n",
    "\n",
    "plt.xticks(np.array(new_time_died)[ticks], [12, 9, 6, 3, 0], rotation=-45, fontsize=11)\n",
    "fig.suptitle(f\"Summary last 12 hours\", weight='bold', fontsize=18)\n",
    "\n",
    "for i, id_ in enumerate(feature_idx):\n",
    "    ax[i].plot(new_time_died, hrv_data_1[:, id_], '-o', label=NAMES[id_], color='red')\n",
    "    ax[i].plot(new_time_alive, hrv_data_2[:, id_], '-o', label=NAMES[id_], color='green')\n",
    "    ax[i].grid(True)\n",
    "    ax[i].legend(loc='upper left', bbox_to_anchor=(1,1))\n",
    "\n",
    "# ax[0].plot(new_time_died, hrv_data_12h['Z-H-0114'][:, 3], '-o', label='lfnu', color='red')\n",
    "# ax[1].plot(new_time_died, hrv_data_12h['Z-H-0114'][:, 24], '-o', label='cvi', color='red')\n",
    "# ax[2].plot(new_time_died, hrv_data_12h['Z-H-0114'][:, 19], '-o', label='mean_hr', color='red')\n",
    "# ax[0].plot(new_time_alive, hrv_data_12h['Z-H-0182'][:, 3], '-o', label='lfnu', color='green')\n",
    "# ax[1].plot(new_time_alive, hrv_data_12h['Z-H-0182'][:, 24], '-o', label='cvi', color='green')\n",
    "# ax[2].plot(new_time_alive, hrv_data_12h['Z-H-0182'][:, 19], '-o', label='mean_hr', color='green')\n",
    "\n",
    "# ax[0].grid(True)\n",
    "# ax[1].grid(True)\n",
    "# ax[2].grid(True)\n",
    "# fig.subplots_adjust(wspace=.3)\n",
    "# ax[0].legend(loc='upper left', bbox_to_anchor=(1,1))\n",
    "# ax[1].legend(loc='upper left', bbox_to_anchor=(1,1))\n",
    "# ax[2].legend(loc='upper left', bbox_to_anchor=(1,1))\n",
    "\n",
    "# ax[0].axhspan(100, 150, facecolor='green', alpha=0.2)\n",
    "# ax[0].axhspan(150, 250, facecolor='orange', alpha=0.2)\n",
    "# ax[0].axhspan(250, 330, facecolor='red', alpha=0.2)\n",
    "# ax[1].axhspan(1.5, 3, facecolor='green', alpha=0.2)\n",
    "# ax[1].axhspan(3, 4, facecolor='orange', alpha=0.2)\n",
    "# ax[1].axhspan(4, 5, facecolor='red', alpha=0.2)\n",
    "# ax[2].axhspan(0, 1, facecolor='green', alpha=0.2)\n",
    "# ax[2].axhspan(1, 1.5, facecolor='orange', alpha=0.2)\n",
    "# ax[2].axhspan(1.5, 3, facecolor='red', alpha=0.2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
