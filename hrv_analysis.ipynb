{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dateutil.rrule import rrule, SECONDLY, MINUTELY, HOURLY\n",
    "from collections import defaultdict\n",
    "from hrvanalysis import remove_outliers, remove_ectopic_beats, \\\n",
    "                        interpolate_nan_values, get_frequency_domain_features, \\\n",
    "                        get_time_domain_features, get_csi_cvi_features\n",
    "\n",
    "from preprocessing.load_raw_vital_signs import *\n",
    "from preprocessing.heart_rate_variability import calculate_hrv, apply_hrv\n",
    "\n",
    "RAW_VITAL_DATA_PATH = \"./DATA/Raw Data/filtered_df_removed_nan_files.parquet\"\n",
    "CLINICAL_DATA = './DATA/Clean Data/IMPALA_Clinical_Data_202308211019_Raw.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select patients on hospital duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_clinical_df(path):\n",
    "    \"\"\" Load clinical data into a Pandas DataFrame. \"\"\"\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "    smaller_df = df[['record_id', 'dis_outcome']]\n",
    "    del df\n",
    "\n",
    "    return smaller_df\n",
    "\n",
    "df = read_clinical_df(CLINICAL_DATA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "alive_ids = []\n",
    "died_ids = []\n",
    "\n",
    "print('===== Select patients if they have at least 48 hours of data =====')\n",
    "\n",
    "for patient, series in df.groupby('record_id'):\n",
    "    if series.shape[0] >= 12:\n",
    "\n",
    "        if series['dis_outcome'].iloc[0] == 1:\n",
    "            alive_ids.append(patient)\n",
    "\n",
    "        elif series['dis_outcome'].iloc[0] == 2:\n",
    "            died_ids.append(patient)\n",
    "\n",
    "print(f'- Number of alive patients that fit criteria: {len(alive_ids)}')\n",
    "print(f'- Number of deceased patients that fit criteria: {len(died_ids)}\\n')\n",
    "\n",
    "alive_ids = np.random.choice(alive_ids, size=15, replace=False)\n",
    "died_ids = np.random.choice(died_ids, size=15, replace=False)\n",
    "\n",
    "print('===== Randomly select 15 patients from both selected groups ======')\n",
    "print(f'Alive patients\\n{alive_ids}\\n')\n",
    "print(f'Deceased patients\\n{died_ids}')\n",
    "\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Alive: B-N-0054, B-N-0091, Z-H-0009, Z-H-0014, Z-H-0211\n",
    "# Died: B-N-0058, B-N-0009, B-N-0089, Z-H-0010, Z-H-0243\n",
    "\n",
    "\n",
    "# data = read_raw_vital_signs(RAW_VITAL_DATA_PATH,\n",
    "#                             batch_size=10000,\n",
    "#                             patient_id=list(np.concatenate((alive_ids, died_ids))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save_patient_dict(data, './DATA/Raw Data/raw_patient_dict_p30')\n",
    "\n",
    "data = load_patient_dict('./DATA/Raw Data/raw_patient_dict_p30')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_windowed_data(df, time_unit='m', time_freq=15):\n",
    "    \"\"\"\n",
    "    Split the data into windows.\n",
    "    :param df: Pandas DataFrame containing the data indexed on timestamps.\n",
    "    :param time_unit: time unit of the data window, e.g. s (seconds), m (minutes).\n",
    "    :param time_freq: number of time units in the data window.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    rrule_time = {'h' : HOURLY, 'm' : MINUTELY, 's' : SECONDLY}\n",
    "    windowed_data = []\n",
    "    \n",
    "    for start in rrule(freq=rrule_time[time_unit], interval=time_freq, dtstart=df.index[0], until=df.index[-1]):\n",
    "        \n",
    "        end = start + pd.Timedelta(time_freq, unit=time_unit)\n",
    "        idx = df.index.to_series().between(start, end)\n",
    "        window = df[idx]\n",
    "        windowed_data.append(window.values.squeeze())\n",
    "\n",
    "    del df, window\n",
    "\n",
    "    return windowed_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = data['Z-H-0308'][['ECGHR', 'datetime']].copy()\n",
    "df = df.set_index('datetime')\n",
    "df = df.sort_index()\n",
    "\n",
    "windowed_data = get_windowed_data(df, time_freq=15)\n",
    "\n",
    "print(windowed_data[0].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform HRV analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Frequency-domain features: 'lf', 'hf', 'lf_hf_ratio', 'lfnu', 'hfnu',\n",
    "                           'total_power', 'vlf'\n",
    "Time-domain features: 'mean_nni', 'sdnn', 'sdsd', 'nni_50', 'pnni_50', 'nni_20',\n",
    "                      'pnni_20', 'rmssd', 'median_nni', 'range_nni', 'cvsd',\n",
    "                      'cvnni', 'mean_hr', 'max_hr', 'min_hr', 'std_hr'\n",
    "Nonlinear-domain features: 'csi', 'cvi', 'Modified_csi'\n",
    "\"\"\"\n",
    "\n",
    "hrv_data = defaultdict(list)\n",
    "threshold = 100\n",
    "errors = 0\n",
    "\n",
    "for patient_id in data:\n",
    "\n",
    "    # Create windowed data\n",
    "    df = data[patient_id][['ECGHR', 'datetime']].copy()\n",
    "    df = df.set_index('datetime')\n",
    "    df = df.sort_index()\n",
    "\n",
    "    windowed_data = get_windowed_data(df, time_freq=15)\n",
    "\n",
    "    for i, window in enumerate(windowed_data):\n",
    "\n",
    "        if np.where(~np.isnan(window), 1, 0).sum() >= threshold:\n",
    "\n",
    "            hrv_features = calculate_hrv(window)\n",
    "\n",
    "            if hrv_features:\n",
    "                print(hrv_features['lfnu'])\n",
    "                hrv_data[patient_id].append(list(hrv_features.values()))\n",
    "                break\n",
    "                continue\n",
    "        \n",
    "        errors += 1\n",
    "\n",
    "        break\n",
    "\n",
    "    break\n",
    "\n",
    "\n",
    "hrv_data = {k : np.array(v) for k, v in hrv_data.items()}\n",
    "\n",
    "\n",
    "\n",
    "print(f'{errors} windows were too small or contained too many NaN values')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Remove rows that contain any NaN value\n",
    "clean_hrv_data = {k : v[~np.isnan(v).any(axis=1)] for k, v in hrv_data.items()}\n",
    "\n",
    "# Normalize data\n",
    "normal_hrv_data = {k : normalize(v, axis=0) for k, v in clean_hrv_data.items()}\n",
    "\n",
    "# All data into 1 array\n",
    "all_hrv_data = np.concatenate([v for v in normal_hrv_data.values()])\n",
    "print(all_hrv_data.shape)\n",
    "\n",
    "normal_hrv_data_alive = np.concatenate([v[-192:, :] for k, v in normal_hrv_data.items() if \\\n",
    "                                       k in alive_ids])\n",
    "\n",
    "normal_hrv_data_died = np.concatenate([v[-192:, :] for k, v in normal_hrv_data.items() if \\\n",
    "                                       k in died_ids])\n",
    "\n",
    "normal_hrv_data_all = np.concatenate((normal_hrv_data_alive, normal_hrv_data_died))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "names = np.array(['lf', 'hf', 'lf_hf_ratio', 'lfnu', 'hfnu', 'total_power', 'vlf',\n",
    "         'mean_nni', 'sdnn', 'sdsd', 'nni_50', 'pnni_50', 'nni_20', 'pnni_20',\n",
    "         'rmssd', 'median_nni', 'range_nni', 'cvsd', 'cvnni', 'mean_hr',\n",
    "         'max_hr', 'min_hr', 'std_hr', 'csi', 'cvi', 'Modified_csi'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare HRV metrics to paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clean_hrv_data = {k : v[~np.isnan(v).any(axis=1)] for k, v in hrv_data.items()}\n",
    "\n",
    "clean_hrv_data_alive = np.concatenate([v[-192:, :] for k, v in clean_hrv_data.items() if \\\n",
    "                                       k in alive_ids])\n",
    "\n",
    "clean_hrv_data_died = np.concatenate([v[-192:, :] for k, v in clean_hrv_data.items() if \\\n",
    "                                       k in died_ids])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, feature in enumerate(clean_hrv_data_alive.T):\n",
    "    \n",
    "    print(names[i])\n",
    "    print(f'IQR = {(np.quantile(feature, 0.75) - np.quantile(feature, 0.25))}')\n",
    "    # print(np.quantile(feature, 0.5))\n",
    "    # print(np.quantile(feature, 0.75))\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform PCA on HRV feature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Perform Principal Component Analysis\n",
    "pca = PCA(n_components=5)\n",
    "pca.fit(normal_hrv_data_all)\n",
    "print(pca.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.imshow(pca.components_, cmap='magma', norm='linear')\n",
    "\n",
    "idx = np.argsort(-pca.components_.sum(axis=0))\n",
    "print(np.sort(pca.components_.sum(axis=0)))\n",
    "print(np.array(names)[idx])\n",
    "\n",
    "plt.colorbar()\n",
    "\n",
    "plt.yticks(range(5), range(1, 6))\n",
    "plt.ylabel('Nth component')\n",
    "plt.xticks(range(len(names)), names, rotation=-90)\n",
    "plt.title('Principal components and variable contribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection: VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "def plot_variance(data, title):\n",
    "    \"\"\"\n",
    "    Plot the variance of the given data.\n",
    "    \"\"\"\n",
    "\n",
    "    selector = VarianceThreshold(threshold=0)\n",
    "    selector.fit(data)\n",
    "\n",
    "    plt.bar(range(7), selector.variances_[:7])\n",
    "    plt.bar(range(7, 23), selector.variances_[7:23])\n",
    "    plt.bar(range(23, 26), selector.variances_[23:26])\n",
    "    plt.xticks(range(len(names)), names, rotation=-90)\n",
    "\n",
    "    plt.ylabel('Variance')\n",
    "    plt.xlabel('HRV features')\n",
    "    plt.title(title)\n",
    "    plt.legend(['Frequency-domain', 'Time-domain', 'Non Linear-domain'],\n",
    "               bbox_to_anchor=(0.55, 0.75))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_variance(normal_hrv_data_alive,\n",
    "              title=f'Variance per HRV feature; 15 patients (Alive), {len(normal_hrv_data_alive)} samples')\n",
    "\n",
    "plot_variance(normal_hrv_data_died,\n",
    "              title=f'Variance per HRV feature; 15 patients (Deceased), {len(normal_hrv_data_died)} samples')\n",
    "\n",
    "plot_variance(normal_hrv_data_all,\n",
    "              title=f'Variance per HRV feature; 30 patients (All), {len(normal_hrv_data_all)} samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selector_alive = VarianceThreshold(threshold=0)\n",
    "selector_alive.fit(normal_hrv_data_alive)\n",
    "\n",
    "selector_died = VarianceThreshold(threshold=0)\n",
    "selector_died.fit(normal_hrv_data_died)\n",
    "\n",
    "selector_all = VarianceThreshold(threshold=0)\n",
    "selector_all.fit(normal_hrv_data_all)\n",
    "\n",
    "plt.bar(np.arange(-0.3, 25.7, 1), selector_alive.variances_, width=0.3, align='center')\n",
    "plt.bar(range(26), selector_died.variances_, width=0.3, align='center')\n",
    "plt.bar(np.arange(0.3, 26.3, 1), selector_all.variances_, width=0.3, align='center')\n",
    "# plt.axhline(0.002, color='red')\n",
    "\n",
    "plt.xticks(range(len(names)), names, rotation=-90)\n",
    "\n",
    "plt.legend(['Alive', 'Deceased', 'All'], bbox_to_anchor=(0.9, 0.98))\n",
    "plt.ylabel('Variance')\n",
    "plt.xlabel('HRV features')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0, 1, 5, 6, 8, 10, 11, 12, 13, 26\n",
    "\n",
    "a_mean = np.array([np.nanmean(v[-192:, :], axis=0) for k, v in normal_hrv_data.items() if \\\n",
    "     k in alive_ids])\n",
    "\n",
    "b_mean = np.array([np.nanmean(v[-192:, :], axis=0) for k, v in normal_hrv_data.items() if \\\n",
    "    k in died_ids])\n",
    "\n",
    "x = np.arange(26)\n",
    "\n",
    "a = plt.boxplot(a_mean,\n",
    "             positions=x-0.15,\n",
    "             widths=0.25,\n",
    "             showfliers=False,\n",
    "             patch_artist=True,\n",
    "             boxprops=dict(facecolor='tab:green', color='black'),\n",
    "             medianprops=dict(color='black'),\n",
    "             showmeans=True,\n",
    "             meanprops=dict(markerfacecolor='black', markeredgecolor='black'))\n",
    "\n",
    "b = plt.boxplot(b_mean,\n",
    "             positions=x+0.15,\n",
    "             widths=0.23,\n",
    "             showfliers=False,\n",
    "             patch_artist=True,\n",
    "             boxprops=dict(facecolor='tab:red', color='black'),\n",
    "             medianprops=dict(color='black'),\n",
    "             showmeans=True,\n",
    "             meanprops=dict(markerfacecolor='black', markeredgecolor='black'))\n",
    "\n",
    "plt.xticks(range(26), names, rotation=-90)\n",
    "\n",
    "plt.legend([a['boxes'][0], b['boxes'][0], a['means'][0]], ['Alive', 'Deceased', 'Mean'])\n",
    "plt.title('Average of 30 patients (15 alive, 15 deceased)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(26)\n",
    "\n",
    "# [:, [0, 1, 5, 6, 8, 10, 11, 12, 13, 16, 18, 25]]\n",
    "\n",
    "a_std = np.array([np.nanstd(v[-192:, :], axis=0) for k, v in normal_hrv_data.items() if \\\n",
    "     k in alive_ids])\n",
    "\n",
    "b_std = np.array([np.nanstd(v[-192:, :], axis=0) for k, v in normal_hrv_data.items() if \\\n",
    "    k in died_ids])\n",
    "\n",
    "a = plt.boxplot(a_std,\n",
    "             positions=x-0.15,\n",
    "             widths=0.25,\n",
    "             showfliers=False,\n",
    "             patch_artist=True,\n",
    "             boxprops=dict(facecolor='tab:green', color='black'),\n",
    "             medianprops=dict(color='black'),\n",
    "             showmeans=True,\n",
    "             meanprops=dict(markerfacecolor='black', markeredgecolor='black'))\n",
    "\n",
    "b = plt.boxplot(b_std,\n",
    "             positions=x+0.15,\n",
    "             widths=0.23,\n",
    "             showfliers=False,\n",
    "             patch_artist=True,\n",
    "             boxprops=dict(facecolor='tab:red', color='black'),\n",
    "             medianprops=dict(color='black'),\n",
    "             showmeans=True,\n",
    "             meanprops=dict(markerfacecolor='black', markeredgecolor='black'))\n",
    "\n",
    "# plt.xticks(x, ['lf', 'hf', 'total\\n_power', 'vlf', 'sdnn', 'nni\\n_50', 'pnni\\n_50',\n",
    "#                'nni\\n_20', 'pnni\\n_20', 'range\\n_nni', 'cvnni', 'modi\\nfied\\n_csi'])\n",
    "\n",
    "plt.xticks(range(26), names, rotation=-90)\n",
    "\n",
    "plt.legend([a['boxes'][0], b['boxes'][0], a['means'][0]], ['Alive', 'Deceased', 'Mean'])\n",
    "plt.title('Standard deviation of 30 patients (15 alive, 15 deceased)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot zero counts per feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.bar(np.arange(-0.4, 25.6, 1),\n",
    "        np.count_nonzero(normal_hrv_data_alive == 0, axis=0),\n",
    "        width=0.4, align='edge', label='Alive (count)')\n",
    "\n",
    "plt.bar(range(26),\n",
    "        np.count_nonzero(normal_hrv_data_died == 0, axis=0),\n",
    "        width=0.4, align='edge', label='Deceased (count)')\n",
    "\n",
    "plt.axhline(len(normal_hrv_data_died), color='tab:red', label='Max')\n",
    "\n",
    "plt.xticks(range(26), names, rotation=-90)\n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel('Variance')\n",
    "plt.xlabel('HRV features')\n",
    "plt.title('Zero count per feature')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot histograms of HRV features (alive vs. died)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))# 0, 3, 5, 6, 12, 17, 19, 22\n",
    "\n",
    "j = 0\n",
    "for i, feature in enumerate(names):\n",
    "\n",
    "    if i in [3, 19, 12, 22, 17, 21, 20, 5]:#[19, 12,  3, 22, 17]: #[3, 5, 12, 17, 19, 22]:#[1, 2, 3, 5, 6, 12, 16, 17, 19, 22]:\n",
    "\n",
    "        plt.subplot(3, 3, j+1)\n",
    "        j += 1\n",
    "\n",
    "        # sns.histplot(normal_hrv_data_alive[:, i],\n",
    "        #              kde=True, bins=50, color='tab:blue')\n",
    "        # sns.histplot(normal_hrv_data_died[:, i],\n",
    "        #              kde=True, bins=50, color='tab:orange')\n",
    "\n",
    "        sns.histplot(normal_hrv_data_alive[:, i][normal_hrv_data_alive[:, i] <= 0.2],\n",
    "                     kde=True, bins=50, color='tab:blue')\n",
    "        sns.histplot(normal_hrv_data_died[:, i][normal_hrv_data_died[:, i] <= 0.2],\n",
    "                     kde=True, bins=50, color='tab:orange')\n",
    "        plt.xlim(-0.01, 0.21)\n",
    "\n",
    "        plt.title(feature)\n",
    "        plt.legend(['Alive', 'Deceased'])\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = sns.histplot(normal_hrv_data_alive[:, 4][normal_hrv_data_alive[:, 4] <= 0.2], kde=True, bins=100,\n",
    "                    color='tab:blue', element=\"step\", label='Alive')\n",
    "b = sns.histplot(normal_hrv_data_died[:, 4][normal_hrv_data_died[:, 4] <= 0.2], kde=True, bins=100,\n",
    "                color='tab:orange', element=\"step\", label='Deceased')\n",
    "\n",
    "plt.title(f'hf (zoomed in)')\n",
    "# plt.xlim(-0.01, 0.21)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, feature in enumerate(names):     #[2, 3, 4, 7, 8, 15, 17, 19, 21, 23, 24]\n",
    "\n",
    "    # if i in [0, 3, 5, 6, 12, 17, 19, 22]: #[2, 9, 14, 16, 18]:\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    a = sns.histplot(normal_hrv_data_alive[:, i], kde=True, bins=100,\n",
    "                    color='tab:blue', element=\"step\", label='Alive')\n",
    "    b = sns.histplot(normal_hrv_data_died[:, i], kde=True, bins=100,\n",
    "                    color='tab:orange', element=\"step\", label='Deceased')\n",
    "\n",
    "    plt.title(f'{feature}')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    a = sns.histplot(normal_hrv_data_alive[:, i][normal_hrv_data_alive[:, i] <= 0.2], kde=True, bins=100,\n",
    "                    color='tab:blue', element=\"step\", label='Alive')\n",
    "    b = sns.histplot(normal_hrv_data_died[:, i][normal_hrv_data_died[:, i] <= 0.2], kde=True, bins=100,\n",
    "                    color='tab:orange', element=\"step\", label='Deceased')\n",
    "\n",
    "    plt.title(f'{feature} (zoomed in)')\n",
    "    plt.xlim(-0.01, 0.21)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mutual_info_score\n",
    "from scipy.special import rel_entr\n",
    "\n",
    "sum_to_1_alive = (normal_hrv_data_alive / normal_hrv_data_alive.sum(axis=0))\n",
    "sum_to_1_died = (normal_hrv_data_died / normal_hrv_data_died.sum(axis=0))\n",
    "\n",
    "info_s = []\n",
    "info_e = []\n",
    "\n",
    "for i in [2, 3, 7, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]:#[2, 9, 14, 16, 18]:\n",
    "\n",
    "    # Mututal information score (higher the more alike, 0 if completely independent)\n",
    "    a = np.histogram(sum_to_1_alive[:, i], bins=100)[0]\n",
    "    b = np.histogram(sum_to_1_died[:, i], bins=100)[0]\n",
    "    s = mutual_info_score(a, b)\n",
    "    info_s.append(s)\n",
    "\n",
    "    # Entropy (lower is more alike, 0 if identical)\n",
    "    e = sum(rel_entr(sum_to_1_alive[:, i], sum_to_1_died[:2490, i]))\n",
    "    info_e.append(e)\n",
    "\n",
    "info_s = np.array(info_s)\n",
    "info_e = np.array(info_e)\n",
    "\n",
    "\n",
    "plt.scatter(range(15), info_s, color='tab:blue')\n",
    "plt.scatter(range(15), info_e, color='tab:orange')\n",
    "# plt.plot(range(5), np.abs(info_s - info_e), color='tab:green')\n",
    "plt.xticks(range(15), np.array(names)[[2, 3, 7, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]], rotation=-90)\n",
    "plt.legend(['Mutual information (low)', 'KL-divergence (high)', 'Difference'])\n",
    "plt.title('Distribution similarity between alive and deceased patients')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_s = []\n",
    "info_e = []\n",
    "\n",
    "for i in range(26):\n",
    "\n",
    "    # Mututal information score (higher the more alike, 0 if completely independent)\n",
    "    a = np.histogram(sum_to_1_alive[:, i], bins=100)[0]\n",
    "    b = np.histogram(sum_to_1_died[:, i], bins=100)[0]\n",
    "    s = mutual_info_score(a, b)\n",
    "    info_s.append(s)\n",
    "\n",
    "    # Entropy (lower is more alike, 0 if identical)\n",
    "    e = sum(rel_entr(sum_to_1_alive[:, i], sum_to_1_died[:2490, i]))\n",
    "    info_e.append(e)\n",
    "\n",
    "info_s = np.array(info_s)\n",
    "info_e = np.array(info_e)\n",
    "\n",
    "\n",
    "plt.scatter(range(26), info_s, color='tab:blue')\n",
    "plt.scatter(range(26), info_e, color='tab:orange')\n",
    "plt.xticks(range(26), names, rotation=-90)\n",
    "plt.legend(['Mutual information (low)', 'KL-divergence (high)', 'Difference'])\n",
    "plt.title('Distribution similarity between alive and deceased patients')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection: Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import LogisticRegression, RidgeCV, LassoCV, SGDClassifier\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "y_alive = np.ones(normal_hrv_data_alive.shape[0])\n",
    "y_died = np.zeros(normal_hrv_data_died.shape[0])\n",
    "\n",
    "X = np.concatenate((normal_hrv_data_alive, normal_hrv_data_died))\n",
    "y = np.concatenate((y_alive, y_died))\n",
    "\n",
    "print(normal_hrv_data_alive.shape)\n",
    "print(normal_hrv_data_died.shape)\n",
    "print(y_alive.shape)\n",
    "print(y_died.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clfs = [LogisticRegression(),\n",
    "        LinearSVC(dual='auto'),\n",
    "        ExtraTreesClassifier(n_estimators=50)]\n",
    "        # SGDClassifier(max_iter=10000)]\n",
    "\n",
    "counter = np.zeros(len(names))\n",
    "\n",
    "for clf in clfs:\n",
    "\n",
    "    print(clf)\n",
    "\n",
    "    fitted_clf = clf.fit(X, y)\n",
    "    model_clf = SelectFromModel(fitted_clf, prefit=True, max_features=10)\n",
    "    print(names[model_clf.get_support()])\n",
    "    counter += model_clf.get_support().astype(int)\n",
    "\n",
    "    rfe_clf = RFE(estimator=clf, n_features_to_select=10)\n",
    "    rfe_clf.fit(X, y)\n",
    "    print(names[rfe_clf.get_support()])\n",
    "    counter += rfe_clf.get_support().astype(int)\n",
    "\n",
    "    print()\n",
    "\n",
    "\n",
    "plt.bar(range(len(names)), counter)\n",
    "plt.xticks(range(len(names)), names, rotation=-90)\n",
    "plt.show()\n",
    "\n",
    "np.argsort(-counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Classifiers\n",
    "lsvc_l1 = LinearSVC(dual=\"auto\", penalty=\"l1\", max_iter=10000)\n",
    "lsvc_l2 = LinearSVC(dual=\"auto\", penalty=\"l2\", max_iter=10000)\n",
    "sgd = SGDClassifier(penalty='l1', max_iter=10000)\n",
    "svc = SVC(kernel='linear', max_iter=-1)\n",
    "random_forest = ExtraTreesClassifier(n_estimators=50)\n",
    "\n",
    "ridge = RidgeCV()\n",
    "lasso = LassoCV(max_iter=50000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lsvc_l1 = lsvc_l1.fit(X, y)\n",
    "model_lsvc_l1 = SelectFromModel(lsvc_l1, prefit=True, max_features=5)\n",
    "print(names[model_lsvc_l1.get_support()])\n",
    "\n",
    "lsvc_l2 = lsvc_l2.fit(X, y)\n",
    "model_lsvc_l2 = SelectFromModel(lsvc_l2, prefit=True, max_features=5)\n",
    "print(names[model_lsvc_l1.get_support()])\n",
    "\n",
    "sgd = sgd.fit(X, y)\n",
    "model_sgd = SelectFromModel(sgd, prefit=True, max_features=5)\n",
    "print(names[model_sgd.get_support()])\n",
    "\n",
    "svc = svc.fit(X, y)\n",
    "model_svc = SelectFromModel(svc, prefit=True, max_features=5)\n",
    "print(names[model_svc.get_support()])\n",
    "\n",
    "random_forest = random_forest.fit(X, y)\n",
    "model_tree = SelectFromModel(random_forest, prefit=True, max_features=5)\n",
    "print(names[model_tree.get_support()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ridge = ridge.fit(X, y)\n",
    "\n",
    "sfm_ridge = SelectFromModel(ridge, max_features=5)\n",
    "print(names[sfm_ridge.get_support()])\n",
    "\n",
    "sfs_forward_ridge = SequentialFeatureSelector(ridge, n_features_to_select=5, direction='forward').fit(X, y)\n",
    "print(names[sfs_forward_ridge.get_support()])\n",
    "\n",
    "sfs_backward_ridge = SequentialFeatureSelector(ridge, n_features_to_select=5, direction='backward').fit(X, y)\n",
    "print(names[sfs_backward_ridge.get_support()])\n",
    "\n",
    "\n",
    "lasso = lasso.fit(X, y)\n",
    "\n",
    "sfm_lasso = SelectFromModel(lasso, max_features=5)\n",
    "print(names[sfm_lasso.get_support()])\n",
    "\n",
    "sfs_forward_lasso = SequentialFeatureSelector(lasso, n_features_to_select=5, direction='forward').fit(X, y)\n",
    "print(names[sfs_forward_lasso.get_support()])\n",
    "\n",
    "sfs_backward_lasso = SequentialFeatureSelector(lasso, n_features_to_select=5, direction='backward').fit(X, y)\n",
    "print(names[sfs_backward_lasso.get_support()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "svc = SVC(kernel=\"linear\")\n",
    "rfe = RFE(estimator=svc, n_features_to_select=5, step=1)\n",
    "rfe.fit(X, y)\n",
    "\n",
    "print(names[rfe.get_support()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sel = VarianceThreshold(threshold=0.004).fit(X, y)\n",
    "print(names[sel.get_support()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "counter = np.zeros(len(names))\n",
    "counter += model_lsvc_l1.get_support().astype(int)\n",
    "counter += model_lsvc_l2.get_support().astype(int)\n",
    "counter += model_sgd.get_support().astype(int)\n",
    "counter += model_svc.get_support().astype(int)\n",
    "counter += model_tree.get_support().astype(int)\n",
    "counter += sfm_ridge.get_support().astype(int)\n",
    "counter += sfs_forward_ridge.get_support().astype(int)\n",
    "counter += sfs_backward_ridge.get_support().astype(int)\n",
    "counter += sfm_lasso.get_support().astype(int)\n",
    "counter += sfs_forward_lasso.get_support().astype(int)\n",
    "counter += sfs_backward_lasso.get_support().astype(int)\n",
    "counter += rfe.get_support().astype(int)\n",
    "counter += sel.get_support().astype(int)\n",
    "\n",
    "print(counter)\n",
    "plt.bar(range(len(names)), counter)\n",
    "plt.xticks(range(len(names)), names, rotation=-90)\n",
    "plt.show()\n",
    "\n",
    "# hf, lf_hf_ratio, lfnu, nni_20, pnni_20, cvsd, mean_hr, std_hr\n",
    "# 1, 2, 3, 11, 12, 13, 17, 19, 22\n",
    "# 0, 3, 5, 6, 12, 17, 19, 22\n",
    "np.argsort(-counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(names[[2, 3, 4, 7, 8, 15, 17, 19, 21, 23, 24]])\n",
    "# print()\n",
    "# print(names[[1, 2, 3, 11, 12, 13, 17, 19, 22]])\n",
    "\n",
    "print([n for n in names[[1, 2, 3, 11, 12, 13, 17, 19, 22]] if \\\n",
    "       n in names[[2, 3, 4, 7, 8, 15, 17, 19, 21, 23, 24]]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
