{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fastparquet import ParquetFile\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "\n",
    "VITAL_DATA_PATH = \"./IMPALA_Clinical_Data/Clean Data/Vital Signs Data/df_merged.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_vital_data(path):\n",
    "    \"\"\" Read vital sign data from Parquet file and convert it to Pandas DataFrame. \"\"\"\n",
    "    pf = ParquetFile(path)\n",
    "    df = pf.to_pandas()\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_vital_sign(df):\n",
    "    \"\"\"\n",
    "    Extract vital sign columns\n",
    "    \"\"\"\n",
    "\n",
    "    df = df[df.columns[df.columns.str.startswith(tuple(['ECGHR', 'ECGRR', 'SPO2', 'record_id']))]]\n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_to_date(df):\n",
    "    \"\"\" Convert date indeces to Pandas Timestamps. \"\"\"\n",
    "\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    return df\n",
    "\n",
    "\n",
    "def assign_patient_to_nan(df, time_interval=48):\n",
    "    \"\"\"\n",
    "    All rows where 'record_id' is NaN are assigned to a patient. Loop through\n",
    "    the DataFrame until a NaN appears. Assign previously found record_id to\n",
    "    current row iff the time between the current and previous row is less than\n",
    "    48 hours, otherwise assign row to next record_id.\n",
    "    \"\"\"\n",
    "\n",
    "    record_id = None\n",
    "    lower_i = 0\n",
    "\n",
    "    for i, row in enumerate(df.iterrows()):\n",
    "\n",
    "        if i == len(df)-1: # Check if current row is the last row\n",
    "            df['record_id'][lower_i:i+1] = record_id # Process previous patient df\n",
    "\n",
    "        curr_record_id = row[1]['record_id']\n",
    "\n",
    "        if pd.isna(record_id): # If no record_id is encountered yet\n",
    "            if pd.isna(curr_record_id): # If current record_id is NaN\n",
    "                continue\n",
    "            else: # Else current record_id is saved\n",
    "                record_id = curr_record_id\n",
    "\n",
    "        else: # If a previous record_id already exists\n",
    "            if record_id == curr_record_id: # If previously found and current record_id match\n",
    "                continue\n",
    "                                                \n",
    "            elif not pd.isna(curr_record_id) and record_id != curr_record_id: # If new record_id is encountered\n",
    "                df['record_id'][lower_i:i] = record_id # Process previous patient df\n",
    "                lower_i = i\n",
    "                record_id = curr_record_id\n",
    "\n",
    "            elif pd.isna(curr_record_id): # If current record_id is NaN\n",
    "\n",
    "                # If time interval > 48 hours, new patient is encountered.\n",
    "                if np.abs(df.iloc[[i-1]].index[0] - row[0]) > pd.Timedelta(hours=time_interval):\n",
    "                    df['record_id'][lower_i:i] = record_id # Process previous patient df\n",
    "                    lower_i = i\n",
    "                    record_id = None\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def sort_dates_per_patient(df):\n",
    "    \"\"\" Sort the dates into chronological order (per patient). \"\"\"\n",
    "\n",
    "    list_of_dfs = [df.sort_index() for _, df in df.groupby(df['record_id'],\n",
    "                                                           observed=True,\n",
    "                                                           sort=False)]\n",
    "\n",
    "    return pd.concat(list_of_dfs)\n",
    "\n",
    "\n",
    "def remove_nan_rows(df):\n",
    "    \"\"\"\n",
    "    Remove rows that contain a nan value.\n",
    "    \"\"\"\n",
    "\n",
    "    nan_idx = list(np.where(df.iloc[:, :-1].isna().any(axis=1) == True)[0])\n",
    "\n",
    "    df = df.reset_index() \n",
    "    df = df.drop(index=nan_idx, axis=0)\n",
    "    df = df.set_index('index')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "vital_df = read_vital_data(VITAL_DATA_PATH)\n",
    "\n",
    "# Remove recruitment, daily and discharge information\n",
    "vital_df = extract_vital_sign(vital_df)\n",
    "\n",
    "# Convert date indeces to panda Timestamps\n",
    "vital_df = convert_to_date(vital_df)\n",
    "\n",
    "# Fill in record_id where it is missing\n",
    "vital_df = assign_patient_to_nan(vital_df)\n",
    "\n",
    "# Per patient, sort the data chronologically\n",
    "vital_df = sort_dates_per_patient(vital_df)\n",
    "\n",
    "# Remove rows that contain any NaN values\n",
    "vital_df = remove_nan_rows(vital_df)\n",
    "print(vital_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize vital signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_vital_signs(df, method='standardize'):\n",
    "    \"\"\"\n",
    "    Normalize or standardize the vital signs data.\n",
    "    \"\"\"\n",
    "\n",
    "    data = df.values[:, :-1]\n",
    "\n",
    "    if method == 'standardize':\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(data)\n",
    "        new_data = scaler.transform(data)\n",
    "\n",
    "\n",
    "    elif method == 'normalize':\n",
    "        new_data = normalize(data)\n",
    "\n",
    "    else:\n",
    "        print('Method unknown')\n",
    "\n",
    "    return new_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vital_data = normalize_vital_signs(vital_df, method='standardize')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_cie_data(filename):\n",
    "    \"\"\" Read pickle file containing the CIE information. \"\"\"\n",
    "\n",
    "    with open(filename, 'rb') as f:\n",
    "        cie_dict = pickle.load(f)\n",
    "\n",
    "    return cie_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cie_dict = load_cie_data('saved_CIE')\n",
    "cpr, picu, death = 0, 0, 0\n",
    "\n",
    "for d in cie_dict.values():\n",
    "    cpr += len(d['cpr'])\n",
    "    picu += len(d['picu'])\n",
    "    death += len(d['death'])\n",
    "\n",
    "print(\"=== Critical Illness Events ===\")\n",
    "print(f\"- CPR  : {cpr}\")\n",
    "print(f\"- PICU : {picu}\")\n",
    "print(f\"- Death: {death}\")\n",
    "print(f\"- Total: {cpr + picu + death}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_cie(cie_dict, record_id, predictive_window):\n",
    "    \"\"\" Check whether a CIE occured within the predictive window. \"\"\"\n",
    "\n",
    "    cie = [0, 0, 0]\n",
    "\n",
    "    # Check whether a CIE even occured\n",
    "    if len([1 for v in cie_dict[record_id].values() if v == []]) == 3:\n",
    "        return cie\n",
    "    \n",
    "    for i, timepoints in enumerate(cie_dict[record_id].values()):\n",
    "        cie[i] += sum([1 for t in predictive_window if t in timepoints])\n",
    "\n",
    "    return cie\n",
    "\n",
    "\n",
    "\n",
    "def sliding_window(vital_df, cie_dict, sample_window_hours=4, predictive_window_hours=1):\n",
    "    \"\"\"\n",
    "    Apply a sliding window over the data and label each window with whether a\n",
    "    CIE occured (and what type). Sample_window indicates the amount of data\n",
    "    samples are picked and predictive_window is the amount of time that is looked\n",
    "    into the future to see whether a CIE falls in that window.\n",
    "    \"\"\"\n",
    "\n",
    "    assert sample_window_hours > 0\n",
    "    assert predictive_window_hours > 0\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "    removed_dfs = 0\n",
    "\n",
    "    for record_id, df in vital_df.groupby(['record_id'], observed=False):\n",
    "        \n",
    "        if df.shape[0] < sample_window_hours + predictive_window_hours:\n",
    "            # Skip dataframe if it is too small\n",
    "            removed_dfs += 1\n",
    "            continue\n",
    "\n",
    "        # Turn df into numpy arrays\n",
    "        df = df.drop(['record_id'], axis=1)\n",
    "        timepoints = df.index.values\n",
    "        data = np.array(df.values)\n",
    "\n",
    "        # Creating time range\n",
    "        begin_time, end_time = timepoints[0], timepoints[-1]\n",
    "        time_range = np.arange(begin_time, end_time+1, np.timedelta64(1, \"h\"))\n",
    "        \n",
    "        if len(time_range) < sample_window_hours + predictive_window_hours:\n",
    "            removed_dfs += 1\n",
    "            continue\n",
    "\n",
    "        time_windows = np.lib.stride_tricks.sliding_window_view(\n",
    "            time_range,\n",
    "            sample_window_hours + predictive_window_hours\n",
    "        )\n",
    "\n",
    "        for window in time_windows:\n",
    "            sample_window = window[:sample_window_hours]\n",
    "            predictive_window = window[-predictive_window_hours:]\n",
    "\n",
    "            # Check whether a cie occured\n",
    "            cies = is_cie(cie_dict, record_id[0], predictive_window)\n",
    "\n",
    "            # Fill data values for time in data index, pad with -1 otherwise\n",
    "            data_sample = np.array([data[i].astype(float) if t in timepoints else \\\n",
    "                                    -np.ones(data[i].shape) \\\n",
    "                                    for i, t in enumerate(sample_window)])\n",
    "\n",
    "            # Add data sample to the dataset\n",
    "            X.append(data_sample)\n",
    "            y.append(cies)\n",
    "\n",
    "\n",
    "    print(f'{removed_dfs} DataFrames were too small')\n",
    "    X = np.array(X) # shape: data samples, sample window size, dimensions\n",
    "    y = np.array(y) # shape: data samples, number of CIE (outputs)\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(vital_df.shape)\n",
    "X, y = sliding_window(vital_df, cie_dict, sample_window_hours=8, predictive_window_hours=4)\n",
    "print(X.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create PyTorch dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VitalSignDataset(Dataset):\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.Tensor(X)\n",
    "        self.y = torch.Tensor(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx, :, :], self.y[idx, :]\n",
    "\n",
    "\n",
    "def create_dataloaders(X, y, batch_size=32, seed=42):\n",
    "    \"\"\"\n",
    "    Shuffle the data, split it into train, val and test set and create dataloaders.\n",
    "\n",
    "    NOTE: train, val and test are split, 0.7, 0.1, 0.2.\n",
    "    \"\"\"\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y,\n",
    "                                                  test_size=0.3,\n",
    "                                                  random_state=seed)\n",
    "\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_val, y_val,\n",
    "                                                    test_size=0.66,\n",
    "                                                    random_state=seed)\n",
    "    \n",
    "    train_set = VitalSignDataset(X_train, y_train)\n",
    "    val_set = VitalSignDataset(X_val, y_val)\n",
    "    test_set = VitalSignDataset(X_test, y_test)\n",
    "\n",
    "    train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    val_dataloader = DataLoader(val_set, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    test_dataloader = DataLoader(test_set, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "    return train_dataloader, val_dataloader, test_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = create_dataloaders(X, y)\n",
    "\n",
    "sample = next(iter(test_dataloader))\n",
    "\n",
    "print(sample[0][0])\n",
    "print(sample[1][0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
